---
title: "Parallel hardware & software"
excerpt: "병렬 프로그래밍 2주차 정리"
# toc: true
# toc_sticky: true
categories: [Parallel programming]
tags: [Parallel programming]
use_math: true
---

> 📢 학교 수업 **병렬프로그래밍기초** 수업에 대한 정리이므로 TMI들이 있을 수 있습니다.

## 폰노이만 아키텍처

- CPU가 메인 메모리와 연결(_bus_)되어 있는 구조
- CPU에는 빠른 연산을 위한 특정한 레지스터가 있고 많은 데이터들은 메인 메모리에 적재

### 폰노이만 구조

- CPU는 **ALU**와 **Control Unit**으로 구성, 레지스터에 저장된 데이터를 처리

  ALU는 실질적으로 명령어을 실행하는 worker(_data path_)

  Control unit은 해당하는 worker들을 하기 위해 제어 신호를 만듦(_boss_)

- 메인 메모리는 많은 데이터(특정 명령어, 데이터)를 저장
- 그 외 레지스터, 프로그램 카운터, 버스가 있음

### 폰 노이만 아키텍처의 병목

메모리의 값을 읽고 쓰는 구조이기 때문에 기억장치에 병목현상이 생길 수 밖에 없다. 이러한 문제를 개선하기 위해 **Memory Hierarchy**(_cache_)가 있다.

---

### 멀티태스킹

- 싱글 프로세서에서 여러 프로그램을 수행하는 것
- 여러 job들을 time slice마다 번갈아가면서 프로세스를 수행시키는 것(_round-robin_)
- concurrent하게 돌아가므로 마치 동시에 돌아가는 것처럼 만드는 것

### 쓰레딩

- 프로세스는 여러 개의 쓰레드를 가질 수 있음
- 하나의 프로그램을 여러 개의 독립적인 work로 나누어 작업할 수 있음
- 쓰레드는 코드, 데이터, 힙은 공유를 하고 스택과 레지스터를 독자적으로 가짐
- 디버깅과 해석의 어려움이 있음
- 하지만 컨텍스트 스위칭 오버헤드가 상대적으로 작기 때문에 실질적으로 훨씬 더 빠름

일반적인 병렬 프로그래밍은 쓰레드를 이용한다.

---

## 폰 노이만 아키텍처의 병목: 메모리

폰 노이만 아키텍처의 병목은 메모리 연산에 있다.

### Cache

- 캐시는 최근에 접근한 메모리들을 cpu 가까이에 위치시킨 것
- 같은 칩 안에 캐시를 둠으로써 일반적인 메모리보다 빠르게 READ/WRITE을 할 수 있음.
- 필요한 데이터를 캐시에 가져다 두는 것은 효율적임

🤔 앞으로 접근할 메모리를 캐시에 두는 것이 중요하지만 어떤 데이터가 사용될 지를 판단하는 방법은?

### Locality (지역성)

- Temporal locality

  프로그래머들이 짠 코드에는 최근에 쓴 데이터들이 가까운 미래에 다시 짤 가능성이 높았다라는 특징이 있다. 따라서 최근에 업데이트한 데이터들을 캐시에 넣는 전략을 사용한다.

- Spatial locality

  접근한 데이터의 근처에 있는 데이터를 쓸 가능성 또한 높다.

### Cache Architecture

- L1 Cache: 프로세서와 가장 가까운 캐시, 속도를 위해 I$와 D$로 나뉜다.

  Instruction cache(I\$): 메모리의 TEXT 영역 데이터를 다루는 캐시

  Data cache(D\$): TEXT 영역을 제외한 모든 데이터를 다루는 캐시

- L2 Cache: 용량이 큰 캐시, 크기를 위해 L1 캐시처럼 나누지 않는다.
- L3 Cache: 멀티 코어 시스템에서 여러 코어가 공유하는 캐시

### Cache Metrics

캐시의 성능을 측정할 때는 **Hit latency**와 **Miss latency**가 중요하다.

CPU가 요청한 데이터가 캐시에 존재하는 경우를 캐시 히트라고 한다. Hit latency는 히트가 발생해 캐싱된 데이터를 가져올 때 소요되는 시간을 의미한다. 반면 요청한 데이터가 캐시에 없을 경우를 캐시 미스라고 하며, Miss latency는 미스가 발생해 상위 캐시에서 데이터를 가져오거나 메모리에서 데이터를 가져올 때 소요되는 시간을 말한다.

평균 접근 시간은 다음과 같다.

$$Miss\ rate = \frac{Cache\ misses}{Cache\ access}$$

$$Average\ Access\ time = Hit\ latency + Miss\ rate \times Miss\ latency$$

캐시의 성능을 높이기 위해서는 캐시의 크기를 줄여 Hit latency를 줄이거나, 캐시의 크기를 늘려 미스 비율을 줄이거나 더 빠른 캐시를 이용해 latency를 줄이는 방법이 있다.

**Cache hit**의 예시

L1 캐시에 찾으려는 값 $x$가 있는 경우

![]({{ site.url }}/assets/images/73 (1).png ){: .align-center }

**Cache miss**의 예시

L1, L2, L3 캐시를 차례대로 확인하며 값이 없을 경우 메인 메모리에서 찾음

이 경우 많은 시간이 걸림

![]({{ site.url }}/assets/images/73 (2).png ){: .align-center }

### Cache Write Policy

캐시 데이터를 읽을 때는 큰 문제가 없지만 데이터를 쓰는 동작이 발생하고 데이터를 변경할 주소가 캐싱된 상태라면 메모리의 데이터가 업데이트되는 대신 캐시의 데이터를 업데이트된다. 이 경우 캐싱된 데이터를 다시 메인 메모리에 써야 한다. 즉, 메인 메모리와 캐시의 값이 일치하지 않는 경우에 대해 처리할 필요가 있으며 이에 관한 두 가지 쓰기 정책이 있다.

### Write through

캐시에 업데이트 하는 순간 메인 메모리에도 값을 업데이트하는 방식

이 방식은 캐시와 메인 메모리가 항상 일치한다. 하지만 이럴 경우 메인 메모리가 느리기 때문에 데이터를 쓸 때마다 메인 메모리에 접근해야 한다. 따라서 속도가 느리고 캐시를 가지는 의미가 떨어진다.

### Write back

캐시만 업데이트하고 나중에 캐시 블록이 교체될 때 메모리의 데이터를 업데이트하는 방식

`dirty`라는 상태 비트라는 만들고 캐시에 값이 업데이트 되었다면 `dirty` 비트에 표시한다. 캐시 블록이 교체될 때 메인 메모리에 다시 write 해주는 방법이다. Write through 정책과 비교하면 메인 메모리에 접근하는 횟수가 상당히 적으므로 속도 면에서 장점이 있다.

### Cache Mapping

- **Direct mapped**

  1:1 매핑 (random access)

- **Full associative**

  캐시에 있는 블락 중 임의의 블락에 매핑(linear access), 실질적으로 구현 불가능

- **N-way set associative**

  캐시 블락을 $n$개의 묶음으로 나눔 해당 세트 임의의 한 군데에 할당

### Replacement Policy

Full associative, N-way set associative는 교체 정책이 필요하다. 일반적으로 **LRU**(Least Recently Used) 또는 Random을 사용한다.

---

## Virtual Memory

가상 메모리가 필요한 이유는 다음과 같다.

1. 프로그램이 굉장히 크거나 굉장히 큰 데이터셋을 요구할 때 모든 메모리들이 메인 메모리에 들어가지 않을 수 있음
2. **멀티 프로세싱을 함**

Spatial, temporal locality를 고려하여 적재해야 한다.

여러 개의 프로그램이 돌다 보니 프로그램 영역(memory space)간에 protection이 필요하다.

### Page

실질적으로 데이터는 물리 메모리에 저장되어 있다. 가상 메모리와 물리 메모리를 매핑하는 단위가 필요한데 그 단위가 페이지이다.

대부분의 프로그램에서 $2^{12}=4,096$ byte

![]({{ site.url }}/assets/images/73 (3).png ){: .align-center }

- 서로 간섭을 하지 않게끔 각 프로그램에 추상화를 제공
- 각 프로그램을 translation하여 메인 메모리에 접근(_VPN_)

![]({{ site.url }}/assets/images/73 (4).png ){: .align-center }

### TLB(Translation-Lookaside Buffer)

가상메모리 방식은 Translation 할 때, 실제로 접근할 때, 총 두 번 메인 메모리에 접근하기 때문에 성능이 매우 느리다. Page table은 VPN, PTN만을 가지고 있는 작은 메모리이기 때문에 이를 캐싱(_TLB_)하면 성능을 개선할 수 있다.

### Page fault

페이지가 메인 메모리에 없으면 swap space에서 찾는다. 이 때 하드 디스크까지 접근해야 하므로 Page fault의 오버 헤드는 어마어마하다.

---

## ILP(Instruction Level Parallelism)

현재의 CPU는 명령어 수준의 병렬성을 제공한다. 명령어는 여러 개의 프로세스 컴포넌트들을 사용하고 있다. 프로세서 컴포넌트들이 모든 시간에 다 사용되고 있지 않고 있다는 점을 여러 개의 명령어에 대해 적용함으로써 동시에 여러 개의 명령어를 처리하게끔 만든 것이다.

## Pipelining

파이프라이닝은 functional unit을 각각의 스테이지로 할당한 다음 동시에 실행되게 만든 것이다. MIPS 기준 5개의 스테이지로 나누고 있고 각각의 스테이지는 쓰는 하드웨어 유닛이 다르다.

![]({{ site.url }}/assets/images/73 (5).png ){: .align-center }

### Multiple Issue

![]({{ site.url }}/assets/images/73 (6).png ){: .align-center }

위의 예시에서 Adder가 2개 있을 경우 (z1, z2), (z3, z4)를 같이 계산하는 방법으로 성능을 2배 향상시킬 수 있다.

- Static: 컴파일 타임에 순서대로 하는 경우
- Dynamic: 런타임에 스케줄링하여 실행하는 경우

현대에는 **Dynamic multiple issue(superscalar)**로 한다. 즉, 하드웨어가 프로세서의 상태를 보고 여러 개를 동시에 진행하는지를 판단하여 적절하게 처리하는 구조로 되어 있다.

### Speculation

시스템의 성능을 향상시키기 위해 동시에 실행할 수 있는 명령어들을 찾아내는 예측 프로세싱을 의미한다.

컴파일러는 가능하면 많은 명령어들을 동시에 수행하고 싶은데 그렇지 못한 경우들이 있다.

ex) 메모리 접근(캐시 미스의 가능성), 분기문(컴파일 타임 때 판단 불가)

그러나 `while`, `for` 같은 경우 예측이 잘 맞는 경우가 있다. 예측을 했는데 맞을 경우 계속 실행하고 아닐 경우 돌아가는 방식이다.

현대 컴퓨터 아키텍처는 speculation operation으로 예측을 많이 하고 있다.

![]({{ site.url }}/assets/images/73 (7).png ){: .align-center }

### Hardware Multithreading

하드웨어 멀티쓰레딩은 여러 개의 쓰레딩을 지원하는 하드웨어이다. 여러 개의 쓰레드를 동시에 실행되는 것은 늘 가능한 것은 아니며 하드웨어 자원이 허락되어야 실행될 수 있다.

### Thread-level parallelism(TLP)

- 여러 개의 쓰레드를 실행할 수 있게끔 지원하는 하드웨어
- 실제로 동시에 여러 개의 쓰레드를 실행

### Hardware multithreading

- Task가 stall 되지 않는 이상 계속 독립적으로 실행

하드웨어 멀티쓰레딩에는 두 가지 방식이 있다.

1. **Fine-grained**

   쓰레드가 정지되면 다른 쓰레드로 넘어가서 실행한다. 그러나 특정한 쓰레드가 오래 실행하면 계속 실행될 수 있으며 자주 스위칭이 일어날 수도 있다.

   일반적으로 여러 개의 멀티 쓰레딩이 라운드 로빈 형태로 돌아가면서 진행되며 stall될 경우 무시하고 넘어간다.

   짧은 stall이나 긴 stall을 모두 넘어갈 수 있다. 그러나 하나의 쓰레드 입장에서는 빠르지 않다.

2. **Coarse-grained**

   어떠한 기간 동안 stall 되어 있으면 스위칭을 하는 방식이다. 그러나 프로세서가 멈춰있을 수 있다. 쓰레드 스위칭에 인한 딜레이는 적다.

   스위칭 오버헤드는 줄이지만 짧은 stall이 있는 경우 기다려야 한다. 일반적으로 start-up 오버헤드가 커진다.

   파이프라이닝 refill 시간이 start-up 시간보다 클 때 유용하다.

3. **SMT(Simultaneous multithreading)**

   현재 아키텍처는 SMT, Fine-grained의 파생된 방식이다. 명령어 수준에서의 쓰레딩을 실행할 수 있게 한 것이다.

---

### Flynn's taxanomy(플린 분류)

- SISD

  폰 노이만 방식의 아키텍처, 하나의 명령어 스트림을 통해서 하나의 데이터 스트림을 처리(일반적인 싱글 코어)

- SIMD

  명령어 스트림은 하나인데 여러 개의 데이터 스트림을 한꺼번에 처리하는 방식, 똑같은 명령어를 쓰는 것이 특징

- MISD

  여러 개의 명령어 스트림이 있는데 하나의 데이터 스트림을 처리

- MIMD

  여러 개의 명령어 스트림 데이터 스트림을 처리, 분산 컴퓨팅에 가까운 컴퓨팅

![]({{ site.url }}/assets/images/73 (8).png ){: .align-center }

## SIMD = Data parallelism

데이터를 나눠서 여러 개의 프로세싱을 하는 것이 SIMD이다. 여러 개의 프로세싱을 적용하는데 여러 데이터 아이템에 같은 명령어를 제공한다.

즉, $n$개의 프로세스가 똑같은 연산을 수행한다.

![]({{ site.url }}/assets/images/73 (9).png ){: .align-center }

위 그림의 15개의 데이터를 4개의 ALU로 나누어 연산할 경우 각 ALU 당 4번의 연산으로 끝낼 수 있다. 그러나 매 순간 똑같은 연산을 하기 때문에 처리할 연산이 없게 되면 프로세스가 놀게 된다.(ALU4)

## SIMD의 단점

- 모든 ALU가 같은 명령어를 처리해야 하며 남는 프로세스는 놀게 됨
- 모든 프로세스가 같이 시작하고 같이 끝나야 함, 동기화 문제가 있음
- 각각의 ALU에서 instruction storage를 가지고 있지 않음
- 많은 데이터를 처리할 때는 유리하지만 데이터가 적거나 분기문이 많을 경우 적절하지 않은 방법

## Vector Processors

SIMD의 대표적인 예시로 array 단위로 처리하는 방식이다.

- **Vector registers**

  벡터 데이터를 저장할 수 있는 레지스터, 동시에 많은 연산을 지원함

- **Vectorized & pipelined functional units**

  똑같은 operation이 벡터 내의 데이터 element에 적용되게 하는 유닛

- **Vector instructions**

  벡터 명령어들

- **Interleaved memory**

  메모리 시스템 또한 다량의 데이터를 읽어오게끔 지원함. 이런 메모리를 Interleaved memory라고 함.

  메모리 뱅크에다가 주소를 부여하여 동시에 읽음.

- **Strided memory access**

  경우에 따라서 메모리에 순서대로 접근하지 않을 경우 비효율적이게 됨.

  ex) _column access_

- **Scater/gatter**

  임의의 장소에 대한 memory access를 할 경우 비효율적임.

## 장점

- 빠르다
- 쓰기 쉽다
- 프로그래머가 코드를 평가하거나 병렬화할 때 손쉬운 장점이 있다
- 벡터 레지스터에 데이터를 한 번에 넣을 수 있다
- 캐시를 한 번에 다 쓸 수 있음

## 단점

- 불규칙한 데이터를 다루기에는 적합하지 않다

  ex) _if 문_

- SIMD에서 처리할 수 있는 프로세서의 개수에 따라 성능이 결정된다.

## GPU

대표적인 SIMD 프로세서

실시간 그래픽 처리를 위한 프로세서

화면 안에 굉장히 많은 픽셀 정보들이 있으며 이러한 데이터를 잘 처리하는 프로세서

그래픽 프로세싱은 파이프라인 컨버터를 통해 픽셀 array를 병렬 처리로 처리함

일반적으로 C 코딩을 하게 되어 있음. 그래픽, 애니메이션은 GPU를 사용한 예시임

## GPGPU

GPU로 영상 처리만 하기에는 그 성능이 너무 아쉬워서 일반적인 컴퓨팅에도 적용할 수 있게끔 처리하는 방식이다.

GPGPU 전에는 AI neural network을 계산하기에 성능이 너무 느려서 사실상 사용하지 못하는 기술이었는데 이를 가능하게 했다.

GPU는 작은 프로그램에는 성능이 좋지 못하다는 단점이 있으며 일반적인 MP와는 다르다.

---

### MIMD

독자적으로 실행할 수 있는 코어가 각자의 일을 한다. 여러 개의 스트림을 각각 실행할 수 있다.

- 프로세서가 inter connection을 통해 메모리를 공유하는 방식이다.
- 각각의 프로세서는 공유 메모리에 접근할 수 있다.
- 프로세서는 shared data로 다른 프로세서와 암묵적으로 통신할 수 있다.
- 현재 대부분 shared 메모리 시스템은 하나 이상의 멀티코어 시스템으로 이루어진다.

![]({{ site.url }}/assets/images/73 (10).png ){: .align-center }

CPU1과 CPU2가 같은 장소를 참조할 수 있으므로 통신할 수 있다.

Inter connect → Bus, butterfly network, etc

---

![]({{ site.url }}/assets/images/73 (11).png ){: .align-center }

## 🤔 UMA (Uniform Memory Access)

> **_메모리 접근이 일정하다_**

Inter connect를 통해 메모리에 접근하기 때문에 메모리에 접근하는 속도가 똑같다.

![]({{ site.url }}/assets/images/73 (12).png ){: .align-center }

## 🙄 NUMA (Non Uniform Memory Access)

> **_어느 위치에 메모리가 있냐에 따라서 걸리는 시간이 다르다_**

위 아키텍처가 확장되면 분산 메모리이다. NUMA는 물리적, 소프트웨어적 컨셉이 될 수 있다.

일반적으로 캐시를 가지고 있는 시스템은 **CC-NUMA**라고 한다.

![]({{ site.url }}/assets/images/73 (13).png ){: .align-center }

CC-NUMA 예시: Cache에 도달하는 속도와 메모리에 도달하는 속도가 다르다.

![]({{ site.url }}/assets/images/73 (14).png ){: .align-center }

## 🤔 Distirbuted Memory System

각각의 프로세서가 독립적인 메모리를 가지고 있음.

다른 CPU의 메모리에 접근하기 위해서는 **send/receive** 커뮤니케이션을 통해 데이터를 접근해야 함.

이런 통신을 위한 인터페이스로 MPI가 있음.

![]({{ site.url }}/assets/images/73 (15).png ){: .align-center }

## Cluster computing

여러 컴퓨터를 묶어 하나의 큰 컴퓨터를 만든 것이다.

클러스터의 노드들은 하나의 컴퓨터들이라고 볼 수 있다.

---

## Interconnection Networks

Shared memory와 Distributed memory, 프로세서와 프로세서, 메모리와 프로세서를 연결하는 고성능 네트워크이다. 짧은 시간에 많은 데이터를 주고 받으면서 병렬 처리를 해야되기 때문에 데이터의 이동도 빨라야 하고 처리도 빨라야 한다. 일반적인 이더넷 네트워크보다 빠르다.

### Shared Memory Interconnects

- Bus interconnect

  **버스**를 기반으로 여러 프로세서와 메모리를 연결한다. 간단한 구조로 되어있어 구현과 확장이 쉽지만 모든 프로세서와 연결되어 있어 많은 프로세서가 달려 있으면 버스의 혼잡도가 증가하여 성능이 나빠진다. 일반적으로 16개 이상이 넘어가면 성능이 안 좋다.

- Switched interconnect

  디바이스 사이에 라우팅 컨트롤을 하기 위해 **스위치**를 사용한다.

  - Crossbar

    모든 디바이스가 동시에 연결되는 것을 허용한다. 버스보다 상당히 빠르다. 디바이스 별로 직접적으로 연결하기 때문에 비싸다.

![]({{ site.url }}/assets/images/73 (16).png ){: .align-center }

Crossbar network: 프로세서와 메모리가 각각 연결되어 있다. 내부의 스위치로 연결을 제어한다.

### Distributed Memory Interconnects

Dstributed memory 시스템은 일반적으로 Shared memory 시스템보다 더 많은 프로세스와 메모리를 가질 수 있도록 만들어졌다. 더 많은 메모리와 프로세스를 연결해야 하므로 Inter connection network가 더욱 중요하다. 버스나 스위치의 경우 소규모 프로세서에 적합하다.

- Direct interconnect

  프로세서 쌍과 메모리 쌍이 직접적으로 연결되어 있음

- Indirect interconnect

  프로세서와 메모리들끼리 직접 연결되어 있지 않고 간접적으로 연결되어 있음

### Direct Interconnect

![]({{ site.url }}/assets/images/73 (17).png ){: .align-center }

### Ring

프로세서들이 반지 형태로 연결되어 있는 구조, 프로세서가 직접 네트워크에 연결되어 있고 네트워크 프로토콜에 의해 제어된다.

### Toroidal mesh

보통 고성능 멀티 프로세스 시스템에서는 프로토콜 베이스로 되어 있다. 정규적인 형태로 직접 연결되어 있다.

- Connectivity
- Bisection Width

  프로세서를 반으로 나누었을 때 동시에 일어날 수 있는 통신의 숫자

![]({{ site.url }}/assets/images/73 (18).png ){: .align-center }

Bisection Width를 판단할 때는 Worst case를 기준으로 판단함. 위의 ring의 경우에는 2

![]({{ site.url }}/assets/images/73 (19).png ){: .align-center }

![]({{ site.url }}/assets/images/73 (20).png ){: .align-center }

![]({{ site.url }}/assets/images/73 (21).png ){: .align-center }

Fully connected network는 $(p-1)!$ 만큼의 연결이 필요하여 소규모 네트워크에만 사용할 수 있다.

bisection width는 각각 $p/2$로 나누어 지므로 총 $p^2/4$이다.

![]({{ site.url }}/assets/images/73 (22).png ){: .align-center }

bisection width: $p/2$

switch: $1+log_2p$

### Indirect Interconnect

- Crossbar
- Omega network(butterfly network)

프로세서 세트로 구성되어 있으며 중간에 링크를 연결해주는 스위치가 있다.

### Crossbar

![]({{ site.url }}/assets/images/73 (23).png ){: .align-center }

![]({{ site.url }}/assets/images/73 (24).png ){: .align-center }

프로세서들이 Crossbar 네트워크로 연결되어 있다. 직접적으로 연결되어 있찌는 않지만 모든 프로세서에 동시에 연결한다.

### Omega Network

![]({{ site.url }}/assets/images/73 (25).png ){: .align-center }

하나의 프로세스가 모든 프로세스를 갈 수 있다. 스위치가 충돌이 있지 않다면 언제든지 모든 프로세서에 연결될 수 있다.

### Latency

소스에서 데이터를 보냈을 때 데이터를 받기 시작할 때의 시간

### Bandwidth

받기 시작한 후 얼마나 빠른 속도로 전달되는지

latency와 bandwidth가 네트워크의 성능을 판단하는 중요한 지표이다.

### Message Transmission Time

$$Message\ transmission\ time = latency+n(length\ of\ message)/bandwidth $$

Direct, Indirect를 구분하지 않고 이 모델로 네트워크의 성능을 판단한다.

---

### Cache Coherence

캐시는 하드웨어가 제어하므로 프로그래머가 직접 제어할 수 없다.

![]({{ site.url }}/assets/images/73 (26).png ){: .align-center }

$z1$의 값이 $4*7=28$이 아닐 수 있다. 캐시 미스가 나서 Core 0의 1번 연산이 나중에 실행될 수 있기 때문에 $z1$은 $4 * 2 = 8$일 수도 있다.

![]({{ site.url }}/assets/images/73 (27).png ){: .align-center }

Snooper라는 캐시 컨트롤러가 버스 안에 있는 모든 시그널을 체크한다. Snooper가 알아서 시그널에 따라서 invalidation을 한다.

Distribued memory에서는 Directory based cache coherence를 사용한다.

<br><br>
